{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# Teach an LLM to do additions\n",
    "Student: HO Quang Phuoc, Master MVA, ENS Paris-Saclay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca18f",
   "metadata": {},
   "source": [
    "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
    "* the tokenizer\n",
    "* the positional embedding\n",
    "\n",
    "Both the model and the dataset are fixed.\n",
    "\n",
    "We are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of our tokenizer and positional embedding, we may change the number of bits. The initial value of 3 is very small.\n",
    "\n",
    "Here I change the number of bits to 9, meaning we are teaching the model to do additions with 9 digit numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:19:36.201324Z",
     "iopub.status.busy": "2025-02-17T20:19:36.201042Z",
     "iopub.status.idle": "2025-02-17T20:19:39.060962Z",
     "shell.execute_reply": "2025-02-17T20:19:39.060059Z",
     "shell.execute_reply.started": "2025-02-17T20:19:36.201301Z"
    },
    "id": "ae993bb9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:42:19.705095Z",
     "iopub.status.busy": "2025-02-17T20:42:19.704789Z",
     "iopub.status.idle": "2025-02-17T20:42:19.708953Z",
     "shell.execute_reply": "2025-02-17T20:42:19.708035Z",
     "shell.execute_reply.started": "2025-02-17T20:42:19.705074Z"
    },
    "id": "OzGh9ahKF17h",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "number_bits = 9\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9\n",
    "\n",
    "log_interval = 200\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "learning_rate = 8e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:42:22.264415Z",
     "iopub.status.busy": "2025-02-17T20:42:22.264142Z",
     "iopub.status.idle": "2025-02-17T20:42:22.268028Z",
     "shell.execute_reply": "2025-02-17T20:42:22.267181Z",
     "shell.execute_reply.started": "2025-02-17T20:42:22.264393Z"
    },
    "id": "t6aC9uNeIR6C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pad_token = \"[PAD]\"\n",
    "eos_token = \"[EOS]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMvT0B-MGBnY",
   "metadata": {
    "id": "BMvT0B-MGBnY"
   },
   "source": [
    "### Baseline: character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "g2QiF-otFur3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:42:23.211517Z",
     "iopub.status.busy": "2025-02-17T20:42:23.211248Z",
     "iopub.status.idle": "2025-02-17T20:42:23.217412Z",
     "shell.execute_reply": "2025-02-17T20:42:23.216587Z",
     "shell.execute_reply.started": "2025-02-17T20:42:23.211494Z"
    },
    "id": "g2QiF-otFur3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v: k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k: v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3gckvebGGYt",
   "metadata": {
    "id": "j3gckvebGGYt"
   },
   "source": [
    "# Implement your tokenizer here!\n",
    "\n",
    "You can do anything (as long as you do not compute the addition!).\n",
    "Some ideas:\n",
    "* reversing numbers left to right\n",
    "* arranging by groups (of, 2, 3,...)\n",
    "* aligning numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df77e3",
   "metadata": {},
   "source": [
    "**Here is my idea for tokenizer**:\n",
    "\n",
    "1. **Reverse the Numbers:**\n",
    "   - Reverse the digits of the numbers.\n",
    "   - Example: \n",
    "     - `num1 = 123456789` → `987654321`\n",
    "     - `num2 = 145` → `541`\n",
    "\n",
    "2. **Align and Pad the Numbers:**\n",
    "   - Pad the numbers with zeros to match a fixed length (`number_bits`, 9 in this case).\n",
    "   - Example: \n",
    "     - `num1 = 987654321` (9 digits, no change).\n",
    "     - `num2 = 541` → `000000541` (padded to 9 digits).\n",
    "\n",
    "3. **Group Digits by Columns:**\n",
    "   - Align the numbers in columns and group digits vertically.\n",
    "   - Example:\n",
    "     ```\n",
    "     9 8 7 6 5 4 3 2 1\n",
    "     0 0 0 0 0 0 5 4 1\n",
    "     ```\n",
    "   - Resulting pairs:\n",
    "     - (5, 9), (4, 8), (1, 7), (0, 6), (0, 5), (0, 4), (0, 3), (0, 2), (0, 1)\n",
    "\n",
    "4. **Treat Pairs as Sets (here I do this by generating pairs in form (min, max)):**\n",
    "   - Treat each pair as a set (order doesn't matter).\n",
    "   - Example: (5, 9) is the same as (9, 5).\n",
    "\n",
    "5. **Create Token Mappings:**\n",
    "   - Assign each unique pair an ID.\n",
    "   - Pairs like (5, 9) and (9, 5) will share the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbc7ad-493b-4f27-b914-0af0a5f58808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:20:32.637775Z",
     "iopub.status.busy": "2025-02-17T21:20:32.637398Z",
     "iopub.status.idle": "2025-02-17T21:20:32.647996Z",
     "shell.execute_reply": "2025-02-17T21:20:32.647116Z",
     "shell.execute_reply.started": "2025-02-17T21:20:32.637744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharacterAlignmentTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer using character alignment representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_digits=9):\n",
    "        # Define special tokens and max_digits\n",
    "        self.pad = \"[PAD]\"\n",
    "        self.eos = \"[EOS]\"\n",
    "        self.max_digits = max_digits\n",
    "\n",
    "        # Initialize vocabulary with digits, \"[PAD]\" and \"[EOS]\"\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\", self.pad, self.eos]\n",
    "\n",
    "        # Init token-to-id and id-to-token mappings\n",
    "        self.token_to_id = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: v for v, i in self.token_to_id.items()}\n",
    "\n",
    "        # Define pair-to-id and id-to-pair mappings\n",
    "        self.pair_to_id = {}\n",
    "        self.id_to_pair = {}\n",
    "        current_id = len(self.vocab)  # Start after single-character tokens\n",
    "\n",
    "        for i in range(10):\n",
    "            for j in range(i, 10):  # Ensures (min, max) order\n",
    "                pair_str = f\"({i},{j})\"\n",
    "                self.pair_to_id[pair_str] = current_id\n",
    "                self.id_to_pair[current_id] = pair_str\n",
    "                current_id += 1\n",
    "\n",
    "        # Merge pair mappings into token dictionaries\n",
    "        self.token_to_id.update(self.pair_to_id)\n",
    "        self.id_to_token.update(self.id_to_pair)\n",
    "\n",
    "        # Total number of tokens\n",
    "        self.ntokens = len(self.token_to_id)\n",
    "\n",
    "        # Regex to remove invalid characters\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "\n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        Removes all characters not in the vocabulary.\n",
    "        \"\"\"\n",
    "        return re.sub(self.pattern, \"\", text)\n",
    "\n",
    "    def align_numbers(self, num1, num2):\n",
    "        \"\"\"\n",
    "        Aligns two numbers for column-wise processing.\n",
    "        After alignment, the pairs of digits are normalized to (min, max)\n",
    "        so that (3,6) and (6,3) are treated the same.\n",
    "        \"\"\"\n",
    "        num1, num2 = str(num1)[::-1], str(num2)[::-1]  # Reverse for right alignment\n",
    "        num1 = num1.ljust(self.max_digits, \"0\")  # Zero-padding\n",
    "        num2 = num2.ljust(self.max_digits, \"0\")\n",
    "\n",
    "        # Create normalized (min, max) digit pairs\n",
    "        return [\n",
    "            f\"({min(int(d1), int(d2))},{max(int(d1), int(d2))})\"\n",
    "            for d1, d2 in zip(num1, num2)\n",
    "        ]\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes numbers and symbols in a column-wise fashion.\n",
    "        If '+' is found, it aligns the two numbers and converts them to token IDs.\n",
    "        \"\"\"\n",
    "        if \"+\" in text:\n",
    "            parts = text.split(\"+\")\n",
    "            if len(parts) != 2:\n",
    "                raise ValueError(\"Invalid input format. Expected 'num1 + num2 ='\")\n",
    "\n",
    "            num1, num2 = parts[0].strip(), parts[1].split(\"=\")[0].strip()\n",
    "            aligned_numbers = self.align_numbers(num1, num2)\n",
    "\n",
    "            tokens = [\n",
    "                self.token_to_id.get(pair, self.token_to_id[self.pad])\n",
    "                for pair in aligned_numbers\n",
    "            ]\n",
    "            tokens.append(self.token_to_id[\"+\"])\n",
    "            tokens.append(self.token_to_id[\"=\"])\n",
    "            return tokens\n",
    "\n",
    "        # Handle single number sequences\n",
    "        # If c is out of vocab, return \"[PAD]\"\n",
    "        return [self.token_to_id.get(c, self.token_to_id[self.pad]) for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encodes the input text into a list of token IDs.\n",
    "        \"\"\"\n",
    "        return self.pre_tokenization(self.clean(text))\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs back into a string.\n",
    "        \"\"\"\n",
    "        return \"\".join(self.id_to_token.get(x, self.pad) for x in token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b43766-0231-42d5-a231-f04e34437c34",
   "metadata": {},
   "source": [
    "Here I try with some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c900a-d7d1-426f-be10-ea593bd7ec52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:20:37.725597Z",
     "iopub.status.busy": "2025-02-17T21:20:37.725315Z",
     "iopub.status.idle": "2025-02-17T21:20:37.730980Z",
     "shell.execute_reply": "2025-02-17T21:20:37.730111Z",
     "shell.execute_reply.started": "2025-02-17T21:20:37.725577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens in my tokenizer = 69\n",
      "[58, 52, 30, 20, 19, 18, 17, 16, 15, 10, 11]\n",
      "(5,9)(4,8)(1,7)(0,6)(0,5)(0,4)(0,3)(0,2)(0,1)+=\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterAlignmentTokenizer(number_bits)\n",
    "ntokens = tokenizer.ntokens\n",
    "print(f\"number of tokens in my tokenizer = {ntokens}\")\n",
    "\n",
    "prompt = \"123456789 + 145 =\"\n",
    "input_encoded = tokenizer.encode(prompt)\n",
    "output_decoded = tokenizer.decode(input_encoded)\n",
    "print(input_encoded)\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a822e665-3fef-4c00-8d9b-fec784865929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:20:39.465416Z",
     "iopub.status.busy": "2025-02-17T21:20:39.465138Z",
     "iopub.status.idle": "2025-02-17T21:20:39.470367Z",
     "shell.execute_reply": "2025-02-17T21:20:39.469622Z",
     "shell.execute_reply.started": "2025-02-17T21:20:39.465395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 37, 34, 27, 19, 14, 14, 14, 14, 10, 11]\n",
      "(3,5)(2,6)(2,3)(1,4)(0,5)(0,0)(0,0)(0,0)(0,0)+=\n"
     ]
    }
   ],
   "source": [
    "prompt = \"1265 + 54323 =\"\n",
    "input_encoded = tokenizer.encode(prompt)\n",
    "output_decoded = tokenizer.decode(input_encoded)\n",
    "print(input_encoded)\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:21:04.636272Z",
     "iopub.status.busy": "2025-02-17T21:21:04.635978Z",
     "iopub.status.idle": "2025-02-17T21:21:04.643147Z",
     "shell.execute_reply": "2025-02-17T21:21:04.642246Z",
     "shell.execute_reply.started": "2025-02-17T21:21:04.636248Z"
    },
    "id": "daa90f31",
    "outputId": "3e8719ee-d8fa-4984-8b51-4db3457f7dbc",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('294119799+791156163=', '1085275962')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(number_bits=number_bits):\n",
    "    \"\"\"\n",
    "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
    "    \"\"\"\n",
    "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    sum_int = a_int + b_int\n",
    "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
    "\n",
    "\n",
    "sample_datapoint(number_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:21:05.579088Z",
     "iopub.status.busy": "2025-02-17T21:21:05.578787Z",
     "iopub.status.idle": "2025-02-17T21:21:06.808003Z",
     "shell.execute_reply": "2025-02-17T21:21:06.807059Z",
     "shell.execute_reply.started": "2025-02-17T21:21:05.579065Z"
    },
    "id": "b6e861d2",
    "outputId": "c88c2226-0546-473c-c296-88a52823886b",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('65075665+358240227=', '423315892'),\n",
       " ('100825935+600741977=', '701567912'),\n",
       " ('763384340+390144737=', '1153529077'),\n",
       " ('207062109+600163507=', '807225616'),\n",
       " ('967088871+53531711=', '1020620582'),\n",
       " ('49086839+553676836=', '602763675'),\n",
       " ('609142974+483183143=', '1092326117'),\n",
       " ('917377039+557345635=', '1474722674'),\n",
       " ('516485983+856480403=', '1372966386'),\n",
       " ('72464960+6116180=', '78581140')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:21:06.809300Z",
     "iopub.status.busy": "2025-02-17T21:21:06.809053Z",
     "iopub.status.idle": "2025-02-17T21:21:06.818681Z",
     "shell.execute_reply": "2025-02-17T21:21:06.817855Z",
     "shell.execute_reply.started": "2025-02-17T21:21:06.809277Z"
    },
    "id": "fee85050",
    "outputId": "f080f4b0-fd76-48d8-d59f-7c118b6e6fe9",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57600, 6400)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size) :]\n",
    "\n",
    "len(data_train), len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d2eb",
   "metadata": {},
   "source": [
    "### Basline: the classical Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91674239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:21:08.715386Z",
     "iopub.status.busy": "2025-02-17T21:21:08.715094Z",
     "iopub.status.idle": "2025-02-17T21:21:08.721317Z",
     "shell.execute_reply": "2025-02-17T21:21:08.720518Z",
     "shell.execute_reply.started": "2025-02-17T21:21:08.715362Z"
    },
    "id": "91674239",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        d_model: the embedding dimension\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding matrix\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(\n",
    "            1\n",
    "        )  # tensor of shape (max_len, 1) representing positions 0, 1, ..., max_len-1\n",
    "\n",
    "        # torch.arrange(0, d_model, 2) = [0, 2, 4, ..., d_model-2]\n",
    "        # -math.log(10000.0) / d_model = scaling factor\n",
    "        # torch.exp = apply the exponential func\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(\n",
    "            position * div_term\n",
    "        )  # apply sine function to even indies\n",
    "        pe[:, 1::2] = torch.cos(\n",
    "            position * div_term\n",
    "        )  # apply cosine function to odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(\n",
    "            0, 1\n",
    "        )  # add batch dimension, pe shape: (1, max_len, d_model)\n",
    "        self.register_buffer(\n",
    "            \"pe\", pe\n",
    "        )  # pe as buffer, it will be save but won't be updated\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[: x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ceb2",
   "metadata": {},
   "source": [
    "# Implement your positional embedding here!\n",
    "\n",
    "You can do anything. Some ideas:\n",
    "* RoPE\n",
    "* (randomised) FIRE\n",
    "* Abacus\n",
    "\n",
    "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
    "(length_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c31f80-fd65-4506-b5c0-c9350caa113f",
   "metadata": {},
   "source": [
    "**Here I implement positional embedding using Rotary Position Embedding (RoPE) Algorithm**\n",
    "1. Precompute Rotation Frequencies\n",
    "\n",
    "$$\n",
    "\\theta_i = 10000^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d$ is the embedding dimension (must be even).\n",
    "- $i$ is the index over the half-dimensional space $i \\in \\left[0, \\frac{d}{2} \\right]$.\n",
    "\n",
    "From this, we compute the rotation terms:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta p), \\quad \\sin(\\theta p)\n",
    "$$\n",
    "\n",
    "where $p$ is the position index.\n",
    "\n",
    "\n",
    "2. Each embedding vector $x$ of shape $(\\text{batch}, \\text{seq\\_len}, d)$ is split into two equal halves:\n",
    "\n",
    "$$\n",
    "(x_1, x_2) = x[:, :, :\\frac{d}{2}], \\quad x[:, :, \\frac{d}{2}:]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_1$ and $x_2$ each have shape $(\\text{batch}, \\text{seq\\_len}, d/2)$.\n",
    "- The first half $x_1$ represents even-indexed dimensions.\n",
    "- The second half $x_2$ represents odd-indexed dimensions.\n",
    "\n",
    "\n",
    "3. We now rotate the vector components using the precomputed rotation frequencies:\n",
    "\n",
    "$$\n",
    "x_1^{\\text{new}} = x_1 \\cos(\\theta p) - x_2 \\sin(\\theta p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2^{\\text{new}} = x_1 \\sin(\\theta p) + x_2 \\cos(\\theta p)\n",
    "$$\n",
    "\n",
    "\n",
    "4. Merging the Rotated Components\n",
    "\n",
    "$$\n",
    "x^{\\text{new}} = \\left[ x_1^{\\text{new}}, x_2^{\\text{new}} \\right]\n",
    "$$\n",
    "\n",
    "which restores the original embedding shape $(\\text{batch}, \\text{seq\\_len}, d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53f439-fb91-42c0-a196-c8341cebe874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:49.091832Z",
     "iopub.status.busy": "2025-02-17T21:22:49.091496Z",
     "iopub.status.idle": "2025-02-17T21:22:49.099088Z",
     "shell.execute_reply": "2025-02-17T21:22:49.098261Z",
     "shell.execute_reply.started": "2025-02-17T21:22:49.091809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(RoPE, self).__init__()\n",
    "        assert d_model % 2 == 0, \"Embedding size (d_model) must be even for RoPE\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_half = d_model // 2  # RoPE applies to half the embedding size\n",
    "\n",
    "        # Compute the frequency terms for rotation\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            -math.log(10000.0) * torch.arange(0, self.d_half, 1).float() / self.d_half\n",
    "        )\n",
    "\n",
    "        # Precompute cosine and sine values\n",
    "        freqs = position * div_term  # (max_len, d_half)\n",
    "        self.register_buffer(\"cos\", freqs.cos())  # (max_len, d_half)\n",
    "        self.register_buffer(\"sin\", freqs.sin())  # (max_len, d_half)\n",
    "\n",
    "    def _apply_rope(self, x, cos, sin):\n",
    "        \"\"\"\n",
    "        Applies rotary position embeddings to the input tensor x.\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            cos: (seq_len, d_half)\n",
    "            sin: (seq_len, d_half)\n",
    "        Returns:\n",
    "            RoPE-applied tensor of the same shape.\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        assert (\n",
    "            d_model == self.d_model\n",
    "        ), \"Input embedding size does not match model embedding size\"\n",
    "\n",
    "        # Split into two halves\n",
    "        x1, x2 = x[..., : self.d_half], x[..., self.d_half :]\n",
    "\n",
    "        # Apply rotary transformation\n",
    "        x1_new = x1 * cos - x2 * sin\n",
    "        x2_new = x1 * sin + x2 * cos\n",
    "\n",
    "        # Concatenate back along the last dimension\n",
    "        return torch.cat([x1_new, x2_new], dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (seq_len, batch, d_model)\n",
    "        Returns:\n",
    "            RoPE-applied tensor of the same shape.\n",
    "        \"\"\"\n",
    "        seq_len, batch, d_model = x.shape\n",
    "        assert (\n",
    "            d_model == self.d_model\n",
    "        ), \"Input embedding size does not match RoPE model embedding size\"\n",
    "\n",
    "        # Transpose to (batch, seq_len, d_model) for processing\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Select cos and sin based on seq_len\n",
    "        cos, sin = self.cos[:seq_len], self.sin[:seq_len]  # (seq_len, d_half)\n",
    "\n",
    "        # Apply RoPE and transpose back\n",
    "        x_rotated = self._apply_rope(x, cos.unsqueeze(0), sin.unsqueeze(0))\n",
    "        return x_rotated.transpose(0, 1)  # (seq_len, batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb278ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:49.674606Z",
     "iopub.status.busy": "2025-02-17T21:22:49.674329Z",
     "iopub.status.idle": "2025-02-17T21:22:49.681357Z",
     "shell.execute_reply": "2025-02-17T21:22:49.680537Z",
     "shell.execute_reply.started": "2025-02-17T21:22:49.674586Z"
    },
    "id": "4eb278ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        ntoken = number of unique tokens = vocab size\n",
    "        ninp = embedding size = dim of input embedding\n",
    "        nhead = number of attention heads\n",
    "        nhid = hidden layer size in the feedforward network\n",
    "        nlayers = number of transformer encoder layers\n",
    "        dropout = dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(TransformerModel, self).__init__(\n",
    "            d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers\n",
    "        )\n",
    "        # embedding layer to map token indices to vectors of size ninp\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "        # apply positional embedding\n",
    "        # self.pos_encoder = PositionalEmbedding(ninp, dropout)\n",
    "        self.pos_encoder = RoPE(d_model=ninp)\n",
    "\n",
    "        #  map transformer outputs (size ninp) back to vocabulary tokens\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        # convert token IDs to embedding + scaling\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "\n",
    "        # apply positional embedding\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # feeds the encoded input\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "\n",
    "        # pass encoded ouput through decoder layer\n",
    "        output_dec = self.decoder(output_enc)\n",
    "\n",
    "        \"\"\"\n",
    "        Return:\n",
    "        - log_softmax(output_dec) = prob distribution over tokens\n",
    "        - output_enc = encoder representation\n",
    "        \"\"\"\n",
    "\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "42f9d1ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:50.999145Z",
     "iopub.status.busy": "2025-02-17T21:22:50.998840Z",
     "iopub.status.idle": "2025-02-17T21:22:51.004431Z",
     "shell.execute_reply": "2025-02-17T21:22:51.003420Z",
     "shell.execute_reply.started": "2025-02-17T21:22:50.999122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e093a",
   "metadata": {},
   "source": [
    "Please do not change these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:51.870806Z",
     "iopub.status.busy": "2025-02-17T21:22:51.870485Z",
     "iopub.status.idle": "2025-02-17T21:22:51.954797Z",
     "shell.execute_reply": "2025-02-17T21:22:51.953884Z",
     "shell.execute_reply.started": "2025-02-17T21:22:51.870777Z"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=69, bias=True)\n",
       "  (input_emb): Embedding(69, 128)\n",
       "  (pos_encoder): RoPE()\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntoken=ntokens, ninp=128, nhead=16, nhid=64, nlayers=8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f06e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:52.736085Z",
     "iopub.status.busy": "2025-02-17T21:22:52.735783Z",
     "iopub.status.idle": "2025-02-17T21:22:52.740762Z",
     "shell.execute_reply": "2025-02-17T21:22:52.739932Z",
     "shell.execute_reply.started": "2025-02-17T21:22:52.736061Z"
    },
    "id": "8f2f06e0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens=number_bits + 2):\n",
    "    input_tensor = prompts  # (length_prompts, batch_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor)  # (length_prompts, batch_size, ntokens)\n",
    "        last_output = output[-1, :, :]  # (batch_size, ntokens)\n",
    "        token = torch.argmax(last_output, -1).view((1, -1))  # (1, batch_size)\n",
    "        input_tensor = torch.cat((input_tensor, token), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b7846acd-9b96-4663-aaf7-e9364f5392c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:53.893584Z",
     "iopub.status.busy": "2025-02-17T21:22:53.893307Z",
     "iopub.status.idle": "2025-02-17T21:22:53.968176Z",
     "shell.execute_reply": "2025-02-17T21:22:53.967346Z",
     "shell.execute_reply.started": "2025-02-17T21:22:53.893562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[34, 14, 14, 14, 14, 14, 14, 14, 14, 10, 11, 52, 52, 52, 52, 52, 52, 52,\n",
       "           2,  2,  2,  2]], device='cuda:0'),\n",
       " '(2,3)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)+=(4,8)(4,8)(4,8)(4,8)(4,8)(4,8)(4,8)2222')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1, 1))\n",
    "output = generate(model, prompt_tensor).view((1, -1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:55.586944Z",
     "iopub.status.busy": "2025-02-17T21:22:55.586631Z",
     "iopub.status.idle": "2025-02-17T21:22:55.659078Z",
     "shell.execute_reply": "2025-02-17T21:22:55.658206Z",
     "shell.execute_reply.started": "2025-02-17T21:22:55.586910Z"
    },
    "id": "d76d1b19",
    "outputId": "a1df1dc9-2ecc-4de4-85b2-6bc5bd460439",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[34, 14, 14, 14, 14, 14, 14, 14, 14, 10, 11, 52, 52, 52, 52, 52, 52, 52,\n",
       "           2,  2,  2,  2]], device='cuda:0'),\n",
       " '(2,3)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)+=(4,8)(4,8)(4,8)(4,8)(4,8)(4,8)(4,8)2222')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = generate(model, prompt_tensor).view((1, -1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "00954ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:56.606931Z",
     "iopub.status.busy": "2025-02-17T21:22:56.606596Z",
     "iopub.status.idle": "2025-02-17T21:22:56.611574Z",
     "shell.execute_reply": "2025-02-17T21:22:56.610740Z",
     "shell.execute_reply.started": "2025-02-17T21:22:56.606901Z"
    },
    "id": "00954ddc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list=\"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(\n",
    "                x\n",
    "                + [tokenizer.token_to_id[eos_token]]\n",
    "                + [tokenizer.token_to_id[pad_token]] * (max_length - len(x))\n",
    "            )\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:57.584565Z",
     "iopub.status.busy": "2025-02-17T21:22:57.584262Z",
     "iopub.status.idle": "2025-02-17T21:22:57.590801Z",
     "shell.execute_reply": "2025-02-17T21:22:57.590114Z",
     "shell.execute_reply.started": "2025-02-17T21:22:57.584539Z"
    },
    "id": "2c84beab",
    "outputId": "fc1bea13-d6e1-4a55-b70d-36de00bcec9b",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(1,1)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)+=',\n",
       "  '(1,5)(2,3)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)(0,0)+=',\n",
       "  '(3,4)(2,5)(3,4)(3,5)(4,6)(5,8)(2,9)(3,4)(5,9)+='],\n",
       " ['2[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]',\n",
       "  '56[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]',\n",
       "  '1482408777[EOS]'])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    tokenizer.encode(\"1+1=\"),\n",
    "    tokenizer.encode(\"21+35=\"),\n",
    "    tokenizer.encode(\"549865423+932543354=\"),\n",
    "]\n",
    "answers = [\n",
    "    tokenizer.encode(\"2\"),\n",
    "    tokenizer.encode(\"56\"),\n",
    "    tokenizer.encode(\"1482408777\"),\n",
    "]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [\n",
    "    tokenizer.decode(p) for p in padded_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "264f9227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:58.496452Z",
     "iopub.status.busy": "2025-02-17T21:22:58.496175Z",
     "iopub.status.idle": "2025-02-17T21:22:58.501476Z",
     "shell.execute_reply": "2025-02-17T21:22:58.500686Z",
     "shell.execute_reply.started": "2025-02-17T21:22:58.496431Z"
    },
    "id": "264f9227",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == \"train\" else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:22:59.524256Z",
     "iopub.status.busy": "2025-02-17T21:22:59.523975Z",
     "iopub.status.idle": "2025-02-17T21:22:59.533609Z",
     "shell.execute_reply": "2025-02-17T21:22:59.532729Z",
     "shell.execute_reply.started": "2025-02-17T21:22:59.524235Z"
    },
    "id": "91e281ad",
    "outputId": "22e2d0ee-ede4-41f8-e089-fb63ac2d9787",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11, 64]), torch.Size([11, 64]), 11, 10)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1cfcd10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:23:00.744806Z",
     "iopub.status.busy": "2025-02-17T21:23:00.744469Z",
     "iopub.status.idle": "2025-02-17T21:23:00.750346Z",
     "shell.execute_reply": "2025-02-17T21:23:00.749224Z",
     "shell.execute_reply.started": "2025-02-17T21:23:00.744778Z"
    },
    "id": "1cfcd10a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\n",
    "                \"test\", i\n",
    "            )\n",
    "            prompts = prompts.to(device)  # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(\n",
    "                device\n",
    "            )  # (length_answers + 1, batch_size)\n",
    "            output = generate(\n",
    "                model, prompts, length_answers + 1\n",
    "            )  # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[\n",
    "                length_prompts:, :\n",
    "            ]  # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = (\n",
    "                answers_tokens == target_answers\n",
    "            )  # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:23:01.482347Z",
     "iopub.status.busy": "2025-02-17T21:23:01.482027Z",
     "iopub.status.idle": "2025-02-17T21:23:07.249793Z",
     "shell.execute_reply": "2025-02-17T21:23:07.248723Z",
     "shell.execute_reply.started": "2025-02-17T21:23:01.482319Z"
    },
    "id": "ac335b05",
    "outputId": "b475e943-51b3-401d-d18b-c9d32a49ffb6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638a75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T21:23:19.958734Z",
     "iopub.status.busy": "2025-02-17T21:23:19.958387Z",
     "iopub.status.idle": "2025-02-17T21:23:19.966894Z",
     "shell.execute_reply": "2025-02-17T21:23:19.965903Z",
     "shell.execute_reply.started": "2025-02-17T21:23:19.958702Z"
    },
    "id": "3638a75d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        # print(f\"prompts: {length_prompts}, answers: {length_answers}\")\n",
    "\n",
    "        prompts = prompts.to(device)  # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device)  # (length_answers, batch_size)\n",
    "\n",
    "        input_tensor = torch.cat(\n",
    "            (prompts, target_answers), 0\n",
    "        )  # (length_prompts + length_answers, batch_size)\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, _ = model(\n",
    "            input_tensor\n",
    "        )  # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts - 1 : -1, :, :].reshape(\n",
    "            -1, ntokens\n",
    "        )  # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}\".format(\n",
    "                    batch,\n",
    "                    len(data_train) // batch_size,\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss,\n",
    "                    math.exp(cur_loss),\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def train():\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print(\"-\" * 89)\n",
    "    print(\"| initialisation | test accuracy {:5.2f}\".format(test_accuracy))\n",
    "    print(\"-\" * 89)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # for epoch in range(1, 20+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch()\n",
    "        test_accuracy = evaluate()\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            \"| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}\".format(\n",
    "                epoch, (time.time() - epoch_start_time), test_accuracy\n",
    "            )\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", \"wb\") as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:23:21.140576Z",
     "iopub.status.busy": "2025-02-17T21:23:21.140264Z",
     "iopub.status.idle": "2025-02-17T21:25:10.075232Z",
     "shell.execute_reply": "2025-02-17T21:25:10.074142Z",
     "shell.execute_reply.started": "2025-02-17T21:23:21.140548Z"
    },
    "id": "4e2a8490",
    "outputId": "f70dcac2-5891-4266-8748-85df050f4881",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 22.54 | loss  1.97 | perplexity     7.16\n",
      "|   400/  900 batches | ms/batch 21.94 | loss  1.27 | perplexity     3.57\n",
      "|   600/  900 batches | ms/batch 22.27 | loss  0.50 | perplexity     1.66\n",
      "|   800/  900 batches | ms/batch 22.38 | loss  0.14 | perplexity     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 25.66s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 22.39 | loss  0.06 | perplexity     1.06\n",
      "|   400/  900 batches | ms/batch 22.21 | loss  0.04 | perplexity     1.04\n",
      "|   600/  900 batches | ms/batch 22.06 | loss  0.02 | perplexity     1.02\n",
      "|   800/  900 batches | ms/batch 22.64 | loss  0.02 | perplexity     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 26.13s | test accuracy  1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 22.42 | loss  0.02 | perplexity     1.02\n",
      "|   400/  900 batches | ms/batch 21.77 | loss  0.02 | perplexity     1.02\n",
      "|   600/  900 batches | ms/batch 22.56 | loss  0.02 | perplexity     1.02\n",
      "|   800/  900 batches | ms/batch 22.03 | loss  0.02 | perplexity     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 25.66s | test accuracy  1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 23.74 | loss  0.02 | perplexity     1.02\n",
      "|   400/  900 batches | ms/batch 22.06 | loss  0.01 | perplexity     1.01\n",
      "|   600/  900 batches | ms/batch 21.88 | loss  0.01 | perplexity     1.01\n",
      "|   800/  900 batches | ms/batch 22.07 | loss  0.01 | perplexity     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 25.66s | test accuracy  1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T21:31:49.418429Z",
     "iopub.status.busy": "2025-02-17T21:31:49.418099Z",
     "iopub.status.idle": "2025-02-17T21:31:50.539732Z",
     "shell.execute_reply": "2025-02-17T21:31:50.538750Z",
     "shell.execute_reply.started": "2025-02-17T21:31:49.418398Z"
    },
    "id": "56d9d440",
    "outputId": "1872232b-b120-440b-e1a6-666e079efa3b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: 951112863+218386430= | actual answer: 1169499293    | decode: (0,3)(3,6)(4,8)(2,6)(1,8)(1,3)(1,8)(1,5)(2,9)+=1169499293\n",
      "prompt: 79644146+575846641=  | actual answer: 655490787     | decode: (1,6)(4,4)(1,6)(4,6)(4,4)(6,8)(5,9)(7,7)(0,5)+=655490787\n",
      "prompt: 549047276+473313306= | actual answer: 1022360582    | decode: (6,6)(0,7)(2,3)(3,7)(1,4)(0,3)(3,9)(4,7)(4,5)+=1022360582\n",
      "prompt: 336246734+123438005= | actual answer: 459684739     | decode: (4,5)(0,3)(0,7)(6,8)(3,4)(2,4)(3,6)(2,3)(1,3)+=459684739\n",
      "prompt: 278783775+934186881= | actual answer: 1212970656    | decode: (1,5)(7,8)(7,8)(3,6)(8,8)(1,7)(4,8)(3,7)(2,9)+=1212970656\n",
      "prompt: 860539585+621775351= | actual answer: 1482314936    | decode: (1,5)(5,8)(3,5)(5,9)(3,7)(5,7)(0,1)(2,6)(6,8)+=1482314936\n",
      "prompt: 96041807+617987978=  | actual answer: 714029785     | decode: (7,8)(0,7)(8,9)(1,7)(4,8)(0,9)(6,7)(1,9)(0,6)+=714029785\n",
      "prompt: 516626546+667387943= | actual answer: 1184014489    | decode: (3,6)(4,4)(5,9)(6,7)(2,8)(3,6)(6,7)(1,6)(5,6)+=1184014489\n",
      "prompt: 802191219+351206318= | actual answer: 1153397537    | decode: (8,9)(1,1)(2,3)(1,6)(0,9)(1,2)(1,2)(0,5)(3,8)+=1153397537\n",
      "prompt: 211645586+290862685= | actual answer: 502508271     | decode: (5,6)(8,8)(5,6)(2,5)(4,6)(6,8)(0,1)(1,9)(2,2)+=502508271\n",
      "prompt: 468990031+741065848= | actual answer: 1210055879    | decode: (1,8)(3,4)(0,8)(0,5)(6,9)(0,9)(1,8)(4,6)(4,7)+=1210055879\n",
      "prompt: 265570442+479167283= | actual answer: 744737725     | decode: (2,3)(4,8)(2,4)(0,7)(6,7)(1,5)(5,9)(6,7)(2,4)+=744737725\n",
      "prompt: 767549414+67903166=  | actual answer: 835452580     | decode: (4,6)(1,6)(1,4)(3,9)(0,4)(5,9)(7,7)(6,6)(0,7)+=835452580\n",
      "prompt: 696383771+180994141= | actual answer: 877377912     | decode: (1,1)(4,7)(1,7)(3,4)(8,9)(3,9)(0,6)(8,9)(1,6)+=877377912\n",
      "prompt: 297636257+457768619= | actual answer: 755404876     | decode: (7,9)(1,5)(2,6)(6,8)(3,6)(6,7)(7,7)(5,9)(2,4)+=755404876\n",
      "prompt: 771508094+301948153= | actual answer: 1073456247    | decode: (3,4)(5,9)(0,1)(8,8)(0,4)(5,9)(1,1)(0,7)(3,7)+=1073456247\n",
      "prompt: 239407615+933592703= | actual answer: 1173000318    | decode: (3,5)(0,1)(6,7)(2,7)(0,9)(4,5)(3,9)(3,3)(2,9)+=1173000318\n",
      "prompt: 28416552+646519637=  | actual answer: 674936189     | decode: (2,7)(3,5)(5,6)(6,9)(1,1)(4,5)(6,8)(2,4)(0,6)+=674936189\n",
      "prompt: 649226577+714448960= | actual answer: 1363675537    | decode: (0,7)(6,7)(5,9)(6,8)(2,4)(2,4)(4,9)(1,4)(6,7)+=1363675537\n",
      "prompt: 547005727+132876862= | actual answer: 679882589     | decode: (2,7)(2,6)(7,8)(5,6)(0,7)(0,8)(2,7)(3,4)(1,5)+=679882589\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1, 1))\n",
    "    output = generate(model, prompt_tensor, len(answers)).view((1, -1))\n",
    "    print(\n",
    "        f\"prompt: {prompt:<20} | actual answer: {answers:<13} | decode: {tokenizer.decode(output.tolist()[0]):<20}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ9IOZu8Xo4Y",
   "metadata": {
    "id": "qJ9IOZu8Xo4Y"
   },
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1213",
   "metadata": {},
   "source": [
    "This is just for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "yomPfirhXkLb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T20:55:55.501901Z",
     "iopub.status.busy": "2025-02-17T20:55:55.501466Z",
     "iopub.status.idle": "2025-02-17T20:55:55.508108Z",
     "shell.execute_reply": "2025-02-17T20:55:55.507201Z",
     "shell.execute_reply.started": "2025-02-17T20:55:55.501866Z"
    },
    "id": "yomPfirhXkLb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def data_probing(size):\n",
    "    X = []\n",
    "    y = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n",
    "        _, output = model(input)\n",
    "        output = output[-1, :, :].flatten()\n",
    "        # determine whether there was a carry in the result:\n",
    "        carry = len(data[i][1]) > len(data[i][0]) / 2\n",
    "        X.append(output.cpu().detach().numpy())\n",
    "        y[i] = carry\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "QGmfXVxkppfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-17T20:55:55.768849Z",
     "iopub.status.busy": "2025-02-17T20:55:55.768612Z",
     "iopub.status.idle": "2025-02-17T20:56:02.677521Z",
     "shell.execute_reply": "2025-02-17T20:56:02.676693Z",
     "shell.execute_reply.started": "2025-02-17T20:55:55.768828Z"
    },
    "id": "QGmfXVxkppfP",
    "outputId": "6601c884-004f-40bb-8a1a-71995b17d860",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = data_probing(train_size)\n",
    "X_test, y_test = data_probing(test_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491b4ad-4c10-4f82-967e-1bc65884ca82",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
